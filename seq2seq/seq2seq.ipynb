{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOEz8JjR-gQj",
        "colab_type": "text"
      },
      "source": [
        "# Data preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDSFztmqkoZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import pandas as pd\n",
        "import csv\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WnVchVxaSF9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files = open(\"./drive/My Drive/par_corp.csv\")\n",
        "\n",
        "re_lines = []\n",
        "for line in files:\n",
        "    if line[0] == '[':\n",
        "        continue\n",
        "    re_line = re.sub('[#\".?!\\n]', '', line)\n",
        "    re_lines.append(re_line)\n",
        "\n",
        "kor = []\n",
        "eng = []\n",
        "count = 0\n",
        "for line in re_lines:\n",
        "    count = count + 1\n",
        "    if count % 2 == 0:\n",
        "        kor.append(line)\n",
        "    else:\n",
        "        eng.append(line)\n",
        "\n",
        "d = {'kor':kor, 'eng':eng}\n",
        "par_corp = pd.DataFrame(d)\n",
        "\n",
        "print(par_corp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqcW5goHuRSZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_input, decoder_input, decoder_output = [], [], []\n",
        "\n",
        "# '나는 개와 산책을 하고 있다'\n",
        "# ######## 위 문장의 셀 상태랑 은닉 상태 + <start> 가 인풋으로 들어가면\n",
        "# '<start> i am taking a walk with my dog' -> 각 시점마다 이 문장의 일부분을 decoder_output을 추측하는데 사용하고 있음\n",
        "# 'i am taking a walk with my dog <end>'\n",
        "\n",
        "for stc in par_corp['kor']:\n",
        "    encoder_input.append(stc.split())\n",
        "\n",
        "# 스타트 뒤에 띄어쓰기 있습니다\n",
        "for stc in par_corp['eng']:\n",
        "    decoder_input.append((\"<start> \"+stc).split())\n",
        "\n",
        "# 엔드 앞에 띄어쓰기 있습니다\n",
        "for stc in par_corp['eng']:\n",
        "    decoder_output.append((stc+\" <end>\").split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRtGMp3KvvT1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer_ko = Tokenizer()\n",
        "tokenizer_ko.fit_on_texts(encoder_input)\n",
        "encoder_input = tokenizer_ko.texts_to_sequences(encoder_input)\n",
        "\n",
        "# 만약에 5000이면, 1~4999(패딩하기 전) -> 0~4999(패딩하고 난 뒤)\n",
        "tokenizer_en = Tokenizer()\n",
        "tokenizer_en.fit_on_texts(decoder_input)\n",
        "tokenizer_en.fit_on_texts(decoder_output)\n",
        "decoder_input = tokenizer_en.texts_to_sequences(decoder_input)\n",
        "decoder_output = tokenizer_en.texts_to_sequences(decoder_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1hcwCmuwhtp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 문장 길이 체크\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "len_ko = []\n",
        "for data in encoder_input:\n",
        "    len_ko.append(len(data))\n",
        "\n",
        "len_en = []\n",
        "for data in decoder_input:\n",
        "    len_en.append(len(data))\n",
        "\n",
        "plt.hist(len_ko, label='ko', alpha=0.7)\n",
        "plt.hist(len_en, label='en', alpha=0.7)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5PXZxh2xiCg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# maxlen 없어도 알아서 잘 패딩합니다\n",
        "encoder_input = pad_sequences(encoder_input, padding=\"post\")\n",
        "decoder_input = pad_sequences(decoder_input, padding=\"post\")\n",
        "decoder_output = pad_sequences(decoder_output, padding=\"post\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hll0xCIbaQSv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(encoder_input.shape)\n",
        "print(decoder_input.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0Fjxoc_yEhG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 나중에 prediction 할때 사용하기 위함 (인덱스로 단어 찾기)\n",
        "en_to_index = tokenizer_en.word_index\n",
        "index_to_en = tokenizer_en.index_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tHcgptHyW4B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_size = 12000\n",
        "encoder_input_train = encoder_input[:-test_size]\n",
        "decoder_input_train = decoder_input[:-test_size]\n",
        "decoder_output_train = decoder_output[:-test_size]\n",
        "\n",
        "encoder_input_test = encoder_input[-test_size:]\n",
        "decoder_input_test = decoder_input[-test_size:]\n",
        "decoder_output_test = decoder_output[-test_size:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15K1bAFh9qFD",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2FIv4T0zFJH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\n",
        "from tensorflow.keras.models import Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ju_5mWXqzIiY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 인코더 - 한글 문장 받아서 LSTM 마지막 시점의 은닉상태/셀상태 리턴하도록\n",
        "# 원래는 데이터 갯수랑 문장 길이 같이 들어가야함\n",
        "# 왜 데이터 갯수는 명시하지 않을까요?\n",
        "# fit 할때 validation data -> test set -> 데이터 갯수 다르기 때문에\n",
        "encoder_inputs = Input(shape=(27,)) #27은 문장의 길이\n",
        "# +1을 해서 패딩까지 고려\n",
        "encoder_embed = Embedding(len(tokenizer_ko.word_index)+1, 50)(encoder_inputs)\n",
        "# 패딩 값은 필요없는데... (0에 해당하는 임베딩 벡터 제외)\n",
        "encoder_mask = Masking(mask_value=0)(encoder_embed)\n",
        "# return state를 쓰면 마지막 은닉 상태, 마지막 은닉 상태, 마지막 셀 상태 값을 리턴\n",
        "encoder_outputs, h_state, c_state = LSTM(50, return_state=True)(encoder_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXaPcHYY1BCa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 디코더 - 위에서 리턴한 상태값이랑, 영어 문장 입력받아서 LSTM의 출력값 리턴하도록\n",
        "decoder_inputs = Input(shape=(27,))\n",
        "decoder_embed = Embedding(len(tokenizer_en.word_index)+1, 50)(decoder_inputs)\n",
        "decoder_mask = Masking(mask_value=0)(decoder_embed)\n",
        "# return sequences를 쓰면 전체 시점의 은닉 상태 값을 리턴\n",
        "# 둘 다 쓰면 전체 시점의 은닉 상태(단어갯수만큼)/마지막 은닉 상태/마지막 셀 상태 값을 리턴\n",
        "decoder_lstm = LSTM(50, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_mask, initial_state=[h_state, c_state])\n",
        "decoder_dense = Dense(len(tokenizer_en.word_index)+1, activation='softmax')\n",
        "decoder_softmax_outputs = decoder_dense(decoder_outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBJDA4pQ1W7C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
        "# sparse는 라벨이 정수 형태로 제공될 때 사용되는 함수 (그냥 categorical은 원핫 벡터로 라벨이 제공될 때)\n",
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
        "# 레이어 별로 가중치가 학습되는 것임\n",
        "model.fit(x = [encoder_input_train, decoder_input_train], y = decoder_output_train, validation_data = ([encoder_input_test, decoder_input_test], decoder_output_test), batch_size = 128, epochs = 50)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}